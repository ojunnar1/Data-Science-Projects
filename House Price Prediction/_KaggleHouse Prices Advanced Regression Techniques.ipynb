{"cells":[{"metadata":{},"cell_type":"markdown","source":"## Contents\n<ol>\n    <li>Importing Libraries</li>\n<li> Dataset</li>\n<li>Analyzing distribution of target variable</li>\n<li>Dealing with missing values</li>\n<li>Analyzing distribution of Quantitative features</li>\n<li>Analyzing relationship between qualitative features and target variable</li>\n<li>Correlation Matrix</li>\n<li>Encoding Categorical Variables</li>\n<li>Implementing Feature Engineering</li>\n<li>Applying Machine Learning Models</li>\n<li>Model stacking</li>\n<li>Ensemble Model and converting results to csv file</li></ol>\n    "},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Importing Libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm, skew, kurtosis","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"# Loading the dataset\ntest = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/test.csv\")\ntrain = pd.read_csv(\"../input/house-prices-advanced-regression-techniques/train.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring the training dataset\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.columns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Analyzing distribution of target variable i.e. Sale Price"},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Getting the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.3f} and sigma = {:.3f}\\n'.format(mu, sigma))\n\n# Plotting the distribution\nplt.legend(['Normal Dist. ($\\mu=$ {:.1f} and $\\sigma=$ {:.1f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Skewness** is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.The skewness for a normal distribution is zero, and any symmetric data should have a skewness near zero.\nNegative values for the skewness indicate data that are skewed left and positive values for the skewness indicate data that are skewed right. By skewed left, we mean that the left tail is long relative to the right tail. Similarly, skewed right means that the right tail is long relative to the left tail.\n\n**Kurtosis** is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Skewness & Kurtosis\nprint(\"Skewness before applying log transformation = \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis before applying log transformation = \" + str(train['SalePrice'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Applying Log transformation to improve skewness and kurtosis"},{"metadata":{},"cell_type":"markdown","source":"**Log Transformation **: It helps to fix the skewness proble and brings back the distribution to normal. Log transformation helps to model performance, visualization and also might help to meet the assumptions of statistical tests (for eg. error normality in OLS)"},{"metadata":{"trusted":true},"cell_type":"code","source":"#We will use the np fuction log1p which will apply log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Getting the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.3f} and sigma = {:.3f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal Dist. ($\\mu=$ {:.3f} and $\\sigma=$ {:.3f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice Distribution')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Skewness after after applying  log transformation = \" + str(train['SalePrice'].skew()))\nprint(\"Kurtosis after after applying log transformation = \" + str(train['SalePrice'].kurt()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dealing with Missing values"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Save the 'Id' column\ntrain_Id = train['Id']\ntest_Id = test['Id']\n\n#Now dropping the  'Id' column since it's useless for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\nprint(\"Shape of training data after eliminating Id column : {}\".format(train.shape))\nprint(\"Shape of test data after eliminating Id column : {}\".format(test.shape))\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining the training & test data. Eliminating the ID column and Sale Price column before merging. I also eliminated the outliers present in the sales price as before putting the saleprice in ytrain variable making sure  that it is transformed and free from outliers."},{"metadata":{"trusted":true},"cell_type":"code","source":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\n#Combining train and test data while dropping the Sale Price\ncomb_data= pd.concat((train, test)).reset_index(drop=True)\ncomb_data.drop(['SalePrice'], axis=1 , inplace = True)\nprint(\"Size of combined data is : {}\".format(comb_data.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling features which contain more than 10 % data with None values"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\n\nplt.subplots(0,0, figsize=(20,6))\n\ncomb_data.isnull().mean().sort_values(ascending=False).plot.bar(color='red')\nplt.axhline(y=0.1, color = 'b', linestyle='-')\nplt.title('Before eliminating average missing values per column---->', fontsize=20, weight ='bold')\nplt.show()\n\ncomb_data[\"PoolQC\"] = comb_data[\"PoolQC\"].fillna(\"None\")\ncomb_data[\"MiscFeature\"] = comb_data[\"MiscFeature\"].fillna(\"None\")\ncomb_data[\"Alley\"] = comb_data[\"Alley\"].fillna(\"None\")\ncomb_data[\"FireplaceQu\"] = comb_data[\"FireplaceQu\"].fillna(\"None\")\ncomb_data[\"Fence\"] = comb_data[\"Fence\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\ncomb_data[\"LotFrontage\"] = comb_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\nplt.subplots(0,0, figsize =(20,6))\ncomb_data.isnull().mean().sort_values(ascending=False).plot.bar(color='green')\nplt.axhline(y=0.1, color='b', linestyle='-')\nplt.title('After eliminating average missing values per column ---->', fontsize=20, weight='bold')\nplt.show()\n\n\n                                                                                     \n                                                                            ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_data_na = (comb_data.isnull().sum()/ len(comb_data)) * 100\ncomb_data_na = comb_data_na.drop(comb_data_na[comb_data_na == 0].index).sort_values(ascending=False)[:30]\nmiss_data = pd.DataFrame({\"Ratio of missing values\": comb_data_na})\nmiss_data.head(25)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NAcat = comb_data.select_dtypes(include='object')\nNAquant = comb_data.select_dtypes(exclude = 'object')\nprint(\"There are :\", NAcat.shape[1],\"categorical features with missing values\")\nprint(\"There are :\", NAquant.shape[1],\"quantitative features with missing values\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Filling the columns either with 0 or None depending on the values. Some houses doesn't have garages so I have filled it with 0, meta features like ' GarageQual', 'GarageCond are filled with None. This can be done in many different ways. Here I am using this approach and focus is to completely remove the NaN values from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    comb_data[col] = comb_data[col].fillna('None')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    comb_data[col] = comb_data[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    comb_data[col] = comb_data[col].fillna(0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    comb_data[col] = comb_data[col].fillna('None')\ncomb_data[\"MasVnrType\"] = comb_data[\"MasVnrType\"].fillna(\"None\")\ncomb_data[\"MasVnrArea\"] = comb_data[\"MasVnrArea\"].fillna(0)\ncomb_data['MSZoning'] = comb_data['MSZoning'].fillna(comb_data['MSZoning'].mode()[0])\ncomb_data = comb_data.drop(['Utilities'], axis=1)\ncomb_data[\"Functional\"] = comb_data[\"Functional\"].fillna(\"Typ\")\ncomb_data['Electrical'] = comb_data['Electrical'].fillna(comb_data['Electrical'].mode()[0])\ncomb_data['KitchenQual'] = comb_data['KitchenQual'].fillna(comb_data['KitchenQual'].mode()[0])\ncomb_data['Exterior1st'] = comb_data['Exterior1st'].fillna(comb_data['Exterior1st'].mode()[0])\ncomb_data['Exterior2nd'] = comb_data['Exterior2nd'].fillna(comb_data['Exterior2nd'].mode()[0])\ncomb_data['SaleType'] = comb_data['SaleType'].fillna(comb_data['SaleType'].mode()[0])\ncomb_data['MSSubClass'] = comb_data['MSSubClass'].fillna(\"None\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_data_na = (comb_data.isnull().sum()/ len(comb_data)) * 100\ncomb_data_na = comb_data_na.drop(comb_data_na[comb_data_na == 0].index).sort_values(ascending=False)[:30]\nmiss_data = pd.DataFrame({\"Ratio of missing values\": comb_data_na})\nmiss_data.head(25)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"No missing value found. Data cleaning done successfully."},{"metadata":{},"cell_type":"markdown","source":"## Analyzing distribution of quantitative features"},{"metadata":{"trusted":true},"cell_type":"code","source":"\nquant = train[['1stFlrSF', '2ndFlrSF', '3SsnPorch', 'BedroomAbvGr', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtUnfSF', 'EnclosedPorch', 'Fireplaces', 'FullBath', 'GarageArea', 'GarageCars', 'GarageYrBlt', 'GrLivArea', 'HalfBath', 'KitchenAbvGr', 'LotArea', 'LotFrontage', 'LowQualFinSF', 'MSSubClass', 'MasVnrArea', 'MiscVal', 'MoSold', 'OpenPorchSF', 'OverallCond', 'OverallQual', 'PoolArea', 'ScreenPorch', 'TotRmsAbvGrd', 'TotalBsmtSF', 'WoodDeckSF', 'YearBuilt', 'YearRemodAdd', 'YrSold','SalePrice']]\nquant.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Plotting the features\nquant.hist(figsize = (20,16), bins=50, xlabelsize=10, ylabelsize=10);","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Columns such as 1stFlrSF, GrLivArea, LotFrontage, TotalBSMTSF share similar distribution to SalePrice"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in range(0, len(quant.columns), 5):\n    sns.pairplot(data=quant,\n                x_vars=quant.columns[i:i+5],\n                y_vars=['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can clearly identify some relationships. Most of them seems to have a linear relationship with the SalePrice and if we look closely at the data we can see that a lot of data points are located on x = 0 which may indicate the absence of such feature in the house."},{"metadata":{},"cell_type":"markdown","source":"## Analyzing relationship between qualitative features and target feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(20,20))\nax1 = plt.subplot2grid((6,2),(0,0))\nplt.scatter(x=train['TotalBsmtSF'], y=train['SalePrice'], color=('mediumorchid'), alpha=0.5)\nplt.axvline(x=5900, color='b', linestyle='-')\nplt.title('Basement Area vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(0,1))\nplt.scatter(x=train['GrLivArea'], y=train['SalePrice'], color=('skyblue'),alpha=0.5)\nplt.axvline(x=4600, color='b', linestyle='-')\nplt.title('Ground living Area vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(1,0))\nplt.scatter(x=train['MasVnrArea'], y=train['SalePrice'], color=('gray'),alpha=0.5)\nplt.axvline(x=1500, color='b', linestyle='-')\nplt.title('Masonry veneer Area vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(1,1))\nplt.scatter(x=train['1stFlrSF'], y=train['SalePrice'], color=('crimson'),alpha=0.9)\nplt.axvline(x=4000, color='b', linestyle='-')\nplt.title('First floor Area vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(2,0))\nplt.scatter(x=train['TotRmsAbvGrd'], y=train['SalePrice'], color=('tan'),alpha=0.5)\nplt.axvline(x=13, color='b', linestyle='-')\nplt.title('TotRmsAbvGrd vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(2,1))\nplt.scatter(x=train['GarageArea'], y=train['SalePrice'], color=('orchid'),alpha=0.9)\nplt.axvline(x=1230, color='b', linestyle='-')\nplt.title('GarageArea vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(3,0))\nplt.scatter(x=train['OpenPorchSF'], y=train['SalePrice'], color=('gold'),alpha=0.5)\nplt.axvline(x=470, color='b', linestyle='-')\nplt.title('OpenPorchSF vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(3,1))\nplt.scatter(x=train['EnclosedPorch'], y=train['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=400, color='b', linestyle='-')\nplt.title('EnclosedPorch vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(4,0))\nplt.scatter(x=train['BedroomAbvGr'], y=train['SalePrice'], color=('red'),alpha=0.9)\nplt.axvline(x=7.5, color='b', linestyle='-')\nplt.tight_layout(0.85)\nplt.title('BedroomAbvGr vs. Price', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((6,2),(4,1))\nplt.scatter(x=train['LotArea'], y=train['SalePrice'], color=('yellowgreen'),alpha=0.5)\nplt.axvline(x=90000, color='b', linestyle='-')\nplt.title('LotArea vs. Price', fontsize=15, weight='bold' )\nplt.xticks(weight='bold')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Correlation Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train.corr()\nf, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(corr_matrix, vmax=.8, square=True)\n\n#saleprice correlation matrix\nk = 10 # Nos. of variables\ncols = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"Cor=corr_matrix['SalePrice'].sort_values(ascending=False).head(10).to_frame()\ncm = sns.light_palette(\"seagreen\", as_cmap=True)\n\ncorr_nos = Cor.style.background_gradient(cmap=cm)\ncorr_nos","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looking for null values in the training dataset. Once done, we can proceed towards treating categorical and numerical variables in the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"In statistics it is very important to understand different data types and learn to convert data into a format which the ML model could understand. There are two main types of data i.e.** Numerical(Quantitative) and Categorical(Qualitative)**.\n\nIn numerical we have two sub-data types i.e. Discrete and Continuous data\n\n **Discrete:** Fixed number of integer values. eg. number of cars, pets in house etc.\n\n**Continuous:** It can have infinite number of values. eg. Height of a person, temperature of a room etc.\n\nIn categorical we have two sub-data types i.e. Nominal and Ordinal data\n\n**Nominal:** We can categorize it and order is not important. eg. Gender(Male or Female), Car model (Nissan or Honda)\n\n**Ordinal:** Here order is important. eg. Surveys ( Poor, Satisfactory, Average , Good,  Excellent)\n\nMachine Learning models can only handle numerical values so we have to encode the categorical variables. \nThere are various approaches to encode categorical variables such as :\n\n1) **Label Encoding** -  Here, each category is labeled as ordered integers. Suitable for categorical nominal data\n\n2) **One Hot Encoding** - Here, we transform each categories into individual binary features.Suitable for categorical nominal data\n\n3)**Ordinal Encoding** - We manually assign labels for each value.Suitable for categorical ordinal data.\n\n\nFor this project we are using Label Encoding to encode the categorical variables.\n\n"},{"metadata":{},"cell_type":"markdown","source":"## Encoding Categorical Variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Applying Label Encoding to categorical variables \nfrom sklearn.preprocessing import LabelEncoder\ncat_cols = ('BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor i in cat_cols:\n    label = LabelEncoder()\n    label.fit(list(comb_data[i].values))\n    comb_data[i] = label.transform(list(comb_data[i].values))\n    \n# Checking shape of the dataframe\ncomb_data.shape\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Implementing Feature Engineering\nHere I am adding all floor areas to calculate total area. There are various approaches within feature engineering and it can ultimately produce great results.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_data['CombinedSF'] = comb_data['TotalBsmtSF'] + comb_data['1stFlrSF'] + comb_data['2ndFlrSF']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"comb_data = pd.get_dummies(comb_data)\ncomb_data.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now split the data set into train and test, which is now ready for applying it to the machine learning models."},{"metadata":{"trusted":true},"cell_type":"code","source":"house_train = comb_data[:ntrain]\nhouse_test = comb_data[ntrain:]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Applying Machine Laerning Models"},{"metadata":{"trusted":true},"cell_type":"code","source":"### Importing Machine Learning Libraries\nimport xgboost as xgb\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, train_test_split, cross_val_score\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LassoLarsIC, Lasso, ElasticNet, BayesianRidge\nfrom sklearn.kernel_ridge import KernelRidge\nimport lightgbm as lgb\nfrom sklearn.base import TransformerMixin, clone, BaseEstimator, RegressorMixin","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"### Creating function for checking validation\n## No. of folds\nn_fold = 5\n\ndef rmsle_cross_val(model):\n    kf = KFold(n_fold, shuffle=True, random_state =42).get_n_splits(house_train.values)\n    rmse=np.sqrt(-cross_val_score(model, house_train.values, y_train, scoring='neg_mean_squared_error', cv=kf))\n    return(rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Initiating the different ML models. Selection criteria for efficient model is RMSE error. Model with the lowest RMSE score will be considered. Lasso and Ridge  models wil penalize and prevent the problem of multicollinearity. Lasso performed better as it eliminates the some features from the prediction. Elastic Net was also tuned to a Lasso close model and it gave a low RMSE. Others are applied accordingly."},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Initiating the Models\nElastic_Net = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio= 0.9, random_state=3))\nLso= make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\nG_boost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.005, max_depth=4, max_features='sqrt', min_samples_leaf=15,min_samples_split=5, loss='huber',random_state=5)\nKer_rid= KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nlgb_reg= lgb.LGBMRegressor(objective='regression', num_leaves=5, learning_rate=0.05, n_estimators=720, max_bin=55, bagging_fraction=0.8, bagging_freq=5, feature_fraction=0.2319, feature_fraction_seed=9, bagging_seed=9, min_data_in_leaf=6, min_sum_hessian_in_leaf=11)\nxgb_reg= xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, learning_rate=0.05, max_depth=3, min_child_weight=1.7817,n_estimators=2200, reg_alpha=0.4640, reg_lambda=0.8571,subsample=0.5213, silent=1, random_state=7, nthread= -1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Calculating score of each model\nElnet_score = rmsle_cross_val(Elastic_Net)\nLasso_score = rmsle_cross_val(Lso)\ngboost_score = rmsle_cross_val(G_boost)\nkerrid_score = rmsle_cross_val(Ker_rid)\nlgb_score = rmsle_cross_val(lgb_reg)\nxgb_Score = rmsle_cross_val(xgb_reg)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#### Displaying Scores for each models\nprint(\"Elastic Net : Mean is {:.3f} and std. dev. is {:.3f}\".format(Elnet_score.mean(), Elnet_score.std()))\nprint(\"Lasso regression : Mean is {:.3f} and std. dev. is {:.3f}\".format(Lasso_score.mean(), Lasso_score.std()))\nprint(\"Gradient Boosting : Mean is {:.3f} and std. dev. is {:.3f}\".format(gboost_score.mean(), gboost_score.std()))\nprint(\"Kernel Ridge : Mean is {:.3f} and std. dev. is {:.3f}\".format(kerrid_score.mean(), kerrid_score.std()))\nprint(\"LGBM Regression : Mean is {:.3f} and std. dev. is {:.3f}\".format(lgb_score.mean(), lgb_score.std()))\nprint(\"XGB Regression : Mean is {:.3f} and std. dev. is {:.3f}\".format(xgb_Score.mean(), xgb_Score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## Model Stacking \n### Here we are averaging the base model and building a new class to extend scikit-learn with our model and would also be able to reuse the code."},{"metadata":{"trusted":true},"cell_type":"code","source":"### Mean of models\n\nclass MeanofModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,models):\n        self.models = models\n        \n    # Here we will define the duplicate copy of the original models for fitting the data\n    def fit(self, X, y):\n        self.models_ = [clone(i) for i in self.models]\n            \n        #Training the duplicate models\n        for model in self.models_:\n            model.fit(X,y)\n                \n        return self\n        \n    # We will conduct predictions for the duplicate models and take their mean\n    def predict(self, X):\n        pr = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(pr, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"## Score of mean of models\n### Here, we are taking mean of the following models : Elastic Net, Gradient Boosting, Kernel Ridge and Lasso. We can add any number of models in it. ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mean_models = MeanofModels(models = (Elastic_Net, G_boost, Ker_rid, Lso))\nscore = rmsle_cross_val(mean_models)\nprint(\"Score of the averaged models : Mean is {:.3f} and std. dev. is {:.3f}\".format(score.mean(), score.std()))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Here we can see that even the simplest stacking approach enhances the score . This motivates us to go further and explore a less simple stacking approch.\n\nLess simple Stacking : Adding a Meta-model. In this approach, we will add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nProcedure for the training part can be described as follows:\n\n1) Split the total training set into two disjoint sets (here train and .holdout )\n\n2) Train several base models on the first part (train)\n\n3) Test these base models on the second part (holdout)\n\n4) Use the predictions from 3) known as  out-of-folds predictions as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\n5) The first three steps are done iteratively . If we take for eg. a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\n6) For the prediction part , We will take mean of  the predictions of all base models on the test data and use them as meta-features on which, the final prediction is done with the meta-model.\n\nWe can stack and apply any combination of almost all the models and pick the combination which gives the lowest RMSE using this technique."},{"metadata":{"trusted":true},"cell_type":"code","source":"class StackMeanofModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self,base_m, meta_m, n_folds=5):\n        self.base_m = base_m\n        self.meta_m = meta_m\n        self.n_folds = n_folds\n        \n    #Again ,we will fit the data on duplicate models\n    def fit(self, X, y):\n        self.base_m_ = [list() for i in self.base_m]\n        self.meta_m_ = clone(self.meta_m)\n        kfold = KFold(n_splits = self.n_folds, shuffle =True, random_state =156)\n            \n    ### Now, we will train the duplicated models & then will create predictions out of fold which are required for trainin the meta model\n        outoffold_pred = np.zeros((X.shape[0], len(self.base_m)))\n        for i, model in enumerate(self.base_m):\n            for train_index, holdout_index in kfold.split(X,y):\n                instance = clone(model)\n                self.base_m_[i].append(instance)\n                instance.fit(X[train_index],y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                outoffold_pred[holdout_index,i] = y_pred\n                \n    ### We will train the replica meta model using out of fold predictions as new feature\n        self.meta_m_.fit(outoffold_pred, y)\n        return self\n            \n    ### We will run the base models on test data, generate predcitions and use the mean predictions as meta features for the final prediction which is done by meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([np.column_stack([model.predict(X) for model in base_m]).mean(axis=1)\n        for base_m in self.base_m_])\n        return self.meta_m_.predict(meta_features)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"**Score of Stacked Meaned models:**\n\nFor making the two approaches comparable (by using the same number of models) , we  will just take mean of Elastic Net, Gradient Boost, Kernel ridge and then we add lasso as meta-model."},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_meaned_models = StackMeanofModels(base_m = (Elastic_Net, G_boost, Ker_rid),\n                                                 meta_m = Lso)\n\nscore = rmsle_cross_val(stacked_meaned_models)\nprint(\"Stacked averaged models score: Mean is {:.3f} and std.dev. is{:.3f}\".format(score.mean(), score.std()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stacked_meaned_models.fit(house_train.values, y_train)\nstacked_train_pred = stacked_meaned_models.predict(house_train.values)\nstacked_pred = np.expm1(stacked_meaned_models.predict(house_test.values))\nprint(rmse(y_train, stacked_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_reg.fit(house_train, y_train)\nxgb_train_pred = xgb_reg.predict(house_train)\nxgb_pred = np.expm1(xgb_reg.predict(house_test))\nprint(rmse(y_train, xgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lgb_reg.fit(house_train, y_train)\nlgb_train_pred = lgb_reg.predict(house_train)\nlgb_pred = np.expm1(lgb_reg.predict(house_test.values))\nprint(rmse(y_train, lgb_train_pred))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Ensemble Model and converting results to csv file\n#### Here, we again got a wonderful score by adding a meta learner.Now, we will take the weighted sum of the predictions of the three models to form a ensmeble model and submit the ensemble predictions in a submit.csv file."},{"metadata":{"trusted":true},"cell_type":"code","source":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"###Converting results to .csv file\nsubmit = pd.DataFrame()\nsubmit['Id'] = test_Id\nsubmit['SalePrice'] = ensemble\nsubmit.to_csv('submission.csv',index=False)\nprint(\"Successfully converted results to .csv file\")","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}